<p align="center">
    <img src="../images/data-eng-illus.jpg" alt="Data Engineering Zoomcamp Illustration">
</p>

# Module 1: Containerization and Infrastructure as Code

* Course overview
* Introduction to GCP
* Docker and docker-compose
* Running Postgres locally with Docker
* Setting up infrastructure on GCP with Terraform
* Preparing the environment for the course
* Homework 

## üê≥ Docker & SQL üóÑÔ∏è

The dataset we'll be using is the **Taxi Rides NY dataset**. üöñüìä

---

### 1 - üêã **Introduction to Docker** üêã

Docker is a **powerful platform** that helps developers build, share, and run **containerized applications**. üöÄ Containers are like lightweight, isolated environments that can run applications independently. For data engineers, Docker is a game-changer! üéÆ

**Why Docker?** ü§î  
Imagine running a **data pipeline** in a container. A data pipeline is a process that takes input data, processes it (e.g., cleaning, transforming), and produces output data. For example, a Python script that takes a CSV file, processes it, and stores the results in a Postgres database. üêçüìÇ‚û°Ô∏èüóÑÔ∏è

With Docker, you can run **multiple data pipelines** simultaneously, each in its own container, without any interference. üö¶ You can even run databases like **Postgres** and tools like **pgAdmin** in separate containers, and they can communicate seamlessly. ü§ù


#### üõ†Ô∏è **Why Should Data Engineers Care About Docker?** üõ†Ô∏è

1. **Reproducibility** üîÑ  
   Docker images act as **snapshots** of a container's environment. This ensures that the same code and dependencies run identically across different machines. No more "it works on my machine" issues! üñ•Ô∏è‚úÖ

2. **Local Experiments & Testing** üß™  
   Docker allows you to set up and run **local experiments** and integration tests (CI/CD: Continuous Integration/Continuous Delivery) without installing software directly on your host machine. This makes testing data pipelines in a controlled environment a breeze. üå¨Ô∏è

3. **Cloud Deployment** ‚òÅÔ∏è  
   Docker images can be easily deployed to cloud environments like **Google Cloud Kubernetes**, **AWS Batch**, or serverless platforms like **AWS Lambda** and **Google Cloud Functions**. This ensures consistent code execution in production. üöÄ


#### üöÄ **Getting Started with Docker** üöÄ

Let's create a directory for our project:  
```bash
mkdir 2_docker_sql
```

Inside this directory, we'll create a [**Dockerfile**](./2_docker_sql/Dockerfile). But first, let's get familiar with Docker by running some basic commands:

1. **Hello World** üåç  
   ```bash
   docker run hello-world
   ```

2. **Interactive Ubuntu Container** üêß  
   ```bash
   docker run -it ubuntu bash
   ```
   - `run`: Runs the image.
   - `-it`: Interactive mode.
   - `ubuntu`: The environment we want to run.
   - `bash`: The command to execute in the container.

   üí° **Pro Tip**: Even if you delete everything inside the container (`rm -rf / --no-preserve-root`), the next time you run it, all files will be restored. Containers are **isolated** and don't retain changes between runs. üßº

3. **Python 3.9 Container** üêç  
   ```bash
   docker run -it python:3.9
   ```
   - `3.9`: The tag specifying the Python version.
   - To get into bash, overwrite the entrypoint:
     ```bash
     docker run -it --entrypoint bash python:3.9
     ```


#### üõ†Ô∏è **Creating a Dockerfile** üõ†Ô∏è

Since changes made in a container are discarded after each run, we need a **Dockerfile** to build containers with specific instructions. Here's an [example](./2_docker_sql/Dockerfile):

```docker
FROM python:3.9.1  # Base image üêç

RUN pip install pandas  # Install pandas library üì¶

ENTRYPOINT [ "bash" ]  # Overwrite the entrypoint üö™
```

**Build the Docker Image** üî®  
```bash
docker build -t test:pandas .
```
- `test:pandas`: Image name and version.
- `.`: Build from the Dockerfile in the current directory.

**Run the Docker Image** üèÉ‚Äç‚ôÇÔ∏è  
```bash
docker run -it test:pandas
```
Now, every time you run this image, **pandas** will be available. üêº
You can check the list of Python installed libraries, with `pip list`.


#### üìÇ **Adding a Data Pipeline** üìÇ

Let's create a [Python script](./2_docker_sql/pipeline.py) (`pipeline.py`) and add it to our container. Update the Dockerfile by adding:

```docker
WORKDIR /app  # Set the working directory üìÅ
COPY pipeline.py pipeline.py  # Copy the script into the container üìÑ
```

Now, when you run the container, the current directory will be `/app`, and you'll see your data pipeline script there. üóÇÔ∏è

To make the pipeline run automatically using specified arguments, we will first, update the script to accept **command-line arguments** (`sys.argv`), and then, overwrite the entrypoint to run the script:

```docker
ENTRYPOINT [ "python", "pipeline.py" ]
```

**Run the Image with Arguments** üèÉ‚Äç‚ôÄÔ∏è  
```bash
docker run -it test:pandas 2025-01-25 hello 37
```
All arguments after the image name are passed to the script. üéØ

##### üéâ **Summary** üéâ

Docker is a **must-have tool** for data engineers. It ensures **reproducibility**, simplifies **local testing**, and streamlines **cloud deployment**. With Docker, you can run multiple pipelines and services in isolated environments, making your workflows more efficient and reliable. üöÄ

Now, go ahead and containerize your data pipelines! üê≥‚ú®

---

### 2 - üöï Ingesting NY Taxi Data into Postgres üóÇÔ∏è

Let's dive into running **Postgres in Docker** and building a **data pipeline** using Python scripts to load the NY Taxi dataset into a Postgres database. Here's how to get started:


#### üê≥ Running Postgres in Docker

To run the **Postgres:13** image in Docker, use the following command:

```bash
docker run -it \  # Run the Docker image in interactive mode
   -e POSTGRES_USER="root" \  # Set the username
   -e POSTGRES_PASSWORD="root" \  # Set the password
   -e POSTGRES_DB="ny_taxi" \  # Set the database name
   -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \  # Mount a volume for persistent data storage
   -p 5432:5432 \  # Map port 5432 on the host to port 5432 in the container
   postgres:13  # Specify the Docker image
```

##### Key Points:
- **`-e` flag**: Used to set environment variables (e.g., username, password, database name).
- **Volume Mounting**: Maps a folder on your host machine (`ny_taxi_postgres_data`) to a folder in the container. This ensures data persistence even after the container stops. 
  - üí° **Pro Tip**: Create the folder first with `mkdir ny_taxi_postgres_data`.
- **Port Mapping**: Maps port `5432` on your host to the container's port `5432` to allow SQL queries.


#### üõ†Ô∏è Troubleshooting Empty Folders

If the `ny_taxi_postgres_data` folder appears empty after running the container:
1. **Delete the folder** and run the Docker command again (it will recreate the folder).
2. Adjust folder permissions with:
   ```bash
   sudo chmod a+rwx ny_taxi_postgres_data
   ```
   This ensures you can see the configuration files.

**NB:**
When ingesting data, if you encounter errors because of this folder permissions, you can also either decide to create a `data` folder and put the `ny_taxi_postgres_data` folder inside, or create a local volume instead: `docker volume create --name dtc_postgres_volume_local -d local`. The local volume `dtc_postgres_volume_local` would then be used to replaced our data volume as follows: `-v dtc_postgres_volume_local:/var/lib/postgresql/data`.

#### üîë Accessing the Database with `pgcli`

To interact with the database, we'll use **`pgcli`**, a powerful CLI client for Postgres. Install it with:
```bash
pip install pgcli
```

Then, log into the database:
```bash
pgcli -h localhost -p 5432 -u root -d ny_taxi
```
- **Password**: `root`
- **Commands**:
  - List tables: `\dt`
  - Test the connection: `SELECT 1;`


#### üìä Loading the Dataset with Jupyter Notebooks

We'll use **Jupyter Notebooks** to load the dataset. If you don't have Jupyter installed:
```bash
pip install jupyter
```

Start Jupyter:
```bash
jupyter notebook
```

##### Dataset Details:
- **Yellow Taxi Dataset (Jan 2021)**: [Download Link](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz)
- **Data Dictionary**: [Check Here](https://www.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf)

##### Steps:
1. Use the [upload-data notebook](./2_docker_sql/01-upload-data.ipynb) to download the dataset, create the `yellow_taxi_data` table in the database.
2. Verify the table with `pgcli`:
   - List tables: `\dt;`
   - Describe the table: `\d yellow_taxi_data;`
3. Load data into the table using the [upload-data notebook](./2_docker_sql/01-upload-data.ipynb) and verify with `pgcli`:
   ```sql
   Select Count(*)
   From yellow_taxi_data
   ;
   ```

>>> **Remark:** For building a connection with Postgres databases, we had to install `psycopg`. On *Python 3.12*, we installed `psycopg-binary` and `psycopg2-binary`, and only `psycopg2` on *Python 3.9*.

#### üß™ Testing the Connection without pgcli

You can also use [this notebook](./2_docker_sql/02-pg-test-connection.ipynb) to test the Postgres connection and ensure everything works smoothly.


##### üöÄ Final Notes
- Use **VS Code** or **PyCharm** as alternatives to Jupyter.
- If you don't see the `ny_taxi_data` folder in Jupyter, adjust permissions:
  ```bash
  sudo chmod a+rwx ny_taxi_postgres_data
  ```

---

### 3 - üöÄ Connecting PgAdmin and PostgreSQL üêò

Let's start by running our PostgreSQL database using the following command:  

```bash
docker run -it \
   -e POSTGRES_USER="root" \
   -e POSTGRES_PASSWORD="root" \
   -e POSTGRES_DB="ny_taxi" \ 
   -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \ 
   -p 5432:5432 \  
   postgres:13 
```

Next, let's connect to the database using `pgcli`:

```bash
pgcli -h localhost -p 5432 -u root -d ny_taxi
```

To ensure our data is stored correctly, let's run a simple query: 

```sql
Select MAX(tpep_pickup_datetime), MIN(tpep_pickup_datetime), MAX(total_amount)
From yellow_taxi_data
;
```

#### üé® Why Use PgAdmin?  

While `pgcli` is useful, its command-line interface isn't the most convenient for exploring data. This is where **pgAdmin**, a web-based GUI for PostgreSQL, comes in handy! Instead of installing it manually, we can run it using Docker.  

To start **PgAdmin** in Docker, use the following command:  

```bash
docker run -it \ 
   -e PGADMIN_DEFAULT_EMAIL="admin@admin.com" \ 
   -e PGADMIN_DEFAULT_PASSWORD="root" \ 
   -p 8080:80 \ 
   dpage/pgadmin4
```

##### üîë Understanding the Parameters

- **`PGADMIN_DEFAULT_EMAIL`**: The username to log into PgAdmin.  
- **`PGADMIN_DEFAULT_PASSWORD`**: The password for authentication.  
- **`-p 8080:80`**: Maps port `8080` on our host machine to port `80` in the PgAdmin container, so that requests to `localhost:8080` are forwarded correctly.  

Once this is done, open your browser and visit **`localhost:8080`**. Log in using:  
- **Username**: `admin@admin.com`  
- **Password**: `root`  

#### üîó Connecting PostgreSQL and PgAdmin  

To connect to our database, we need to create a new server:  
1Ô∏è‚É£ Right-click on **Servers** ‚Üí **Register** ‚Üí **Server**  
2Ô∏è‚É£ Set **Name** to `"Local Docker"`  
3Ô∏è‚É£ Under **Connection**, enter:  
   - **Host**: `localhost`  
   - **Username**: `root`  
   - **Password**: `root`  

`localhost` means that the docker image of pgAdmin will be looking for `postgres` inside itself. However, PgAdmin runs inside a separate Docker container, so it can't directly see our PostgreSQL database. To fix this, we need to link both containers within the same network.

##### üåê Creating a Docker Network  

Run the following command to create a network:  

```bash
docker network create pg-network
```

Now, restart both PostgreSQL and PgAdmin while assigning them to this network:  

```bash
docker run -it \
   -e POSTGRES_USER="root" \
   -e POSTGRES_PASSWORD="root" \
   -e POSTGRES_DB="ny_taxi" \ 
   -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \ 
   -p 5432:5432 \ 
   --network=pg-network \ 
   --name pg-database \
   postgres:13 
```

```bash
docker run -it \ 
   -e PGADMIN_DEFAULT_EMAIL="admin@admin.com" \ 
   -e PGADMIN_DEFAULT_PASSWORD="root" \ 
   -p 8080:80 \ 
   --network=pg-network \
   --name pg-admin \
   dpage/pgadmin4
```

##### üèóÔ∏è Key Additions  
- **`--network=pg-network`**: Ensures that both containers are in the same network, so they can communicate.  
- **`--name pg-database`**: Assigns a name to our PostgreSQL container (for pgAdmin to find out how to connect to it).  
- **`--name pg-admin`**: Assigns a name to the PgAdmin container (less critical since nothing connects to it).  

#### ‚úÖ Final Steps  

Go back to PgAdmin, log in, and register a new server:  
- **Name**: `Docker Localhost`  
- **Host**: `pg-database`
- **Maintenance database**: `ny_taxi`
- **Username**: `root`  
- **Password**: `root`  

Now, everything should work smoothly! üéâ We can explore our database visually and even run queries using the built-in query tool.  

üì∏ **Preview:**
![Data First 100 rows](../images/pgAdmin_first_connection.png)

---

### 4 - üê≥ Dockerizing the Ingestion Script üöÄ

We started by using a [Jupyter Notebook](./2_docker_sql/01-upload-data.ipynb) to load data into our **Postgres database**. However, to create a more robust **data pipeline**, we converted the notebook into a Python script. Here's how we did it:

#### üìú Converting Notebook to Script
To convert the notebook into a script, we used the following command:
```bash
jupyter nbconvert --to=script 01-upload-data.ipynb
```
After renaming it, we prepared the [script](./2_docker_sql/ingest_data.py) to **ingest data** (i.e., take data and load it into the database).

#### üß™ Testing the Script
To test the script, we first dropped the existing table in our **PGAdmin server**:
```sql
DROP TABLE yellow_taxi_data;
```
Then, when we tried to count the observations in the table:
```sql
Select Count(*)
From yellow_taxi_data
;
```
We got the error:
```
ERROR:  relation "yellow_taxi_data" does not exist
LINE 1: SELECT COUNT(*) FROM yellow_taxi_data;
```
This error confirms that the table was successfully dropped.

#### üöÄ Running the Ingestion Script
To ingest the data, we ran the following command:
```bash
URL_LINK="https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz"
python ingest_data.py \
   --user=root \
   --password=root \
   --host=localhost \
   --port=5432 \
   --db=ny_taxi \
   --table_name=yellow_taxi_trips \
   --url=${URL_LINK}
```
**Pro Tip:** The command `$?` outputs the error code: `1` (or any non-zero value) for errors and `0` if the program finishes successfully. This is useful for debugging and ensuring your script runs as expected.

**Note:** Passing passwords directly in the command line is not secure, as they can be accessed via terminal history. A better approach is to use **environment variables**.

#### ‚úÖ Verifying Data Ingestion
After running the script, we verified the data was loaded successfully by running:
```sql
SELECT COUNT(*) FROM yellow_taxi_trips;
```
Everything worked smoothly! üéâ

#### üê≥ Dockerizing the Script
Next, we dockerized the ingestion script. We updated our [Dockerfile](./2_docker_sql/Dockerfile) and built the Docker image with:
```bash
docker build -t taxi_ingest:v001 .
```
Then, we ran the Docker container to ingest the data:
```bash
URL_LINK="https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz"
docker run taxi_ingest:v001 \
   --user=root \
   --password=root \
   --host=localhost \
   --port=5432 \
   --db=ny_taxi \
   --table_name=yellow_taxi_trips \
   --url=${URL_LINK}
```
**Pro Tip:** If you forget to use the `-it` option for interactive mode, you can't stop the container with `Ctrl+c`. Instead, use `docker ps` to get the container ID and then stop it with `docker kill <container_id>`.

#### üåê Running in the Same Network
To ensure the Docker container can communicate with the database, we ran it in the same network:
```bash
URL_LINK="https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz"
docker run -it \
   --network=pg-network \
   taxi_ingest:v001 \
    --user=root \
    --password=root \
    --host=pg-database \
    --port=5432 \
    --db=ny_taxi \
    --table_name=yellow_taxi_trips \
    --url=${URL_LINK}
```
**Note:** Specifying the network is crucial for local testing. In production, you'd use a real database URL in the cloud.

#### üöÄ Speeding Up Data Download
To speed up the data download, we hosted the dataset locally using a simple HTTP server:
```bash
python -m http.server
```
This command starts a simple HTTP server on your machine, allowing you to access files in the current directory via your browser at `localhost:8000`. 

To get your machine's IP address, use:
```bash
ifconfig
```
If `ifconfig` is not installed, you can install it with:
```bash
sudo apt-get install net-tools
```
Look for the `inet` address (e.g., `100.91.130.08`). You can now access the dataset saved on your machine via:
```
http://172.28.199.42:8000/yellow_tripdata_2021-01.csv.gz
```
Here's how we ran the Docker container with the local dataset:
```bash
URL_LINK="http://100.91.130.08:8000/yellow_tripdata_2021-01.csv.gz"
docker run -it \
   --network=pg-network \
   taxi_ingest:v001 \
    --user=root \
    --password=root \
    --host=pg-database \
    --port=5432 \
    --db=ny_taxi \
    --table_name=yellow_taxi_trips \
    --url=${URL_LINK}
```
This approach significantly reduced the download time. ‚è©

#### üéØ Final Thoughts
- **Local Testing:** Using `pg-database` and local hosting is great for testing, but in production, you'd use cloud-based databases.
- **Automation:** Instead of running Docker commands manually, consider using **Kubernetes jobs** for automation in production environments.

And that's it! üéâ We successfully dockerized our ingestion script and optimized the data pipeline. üöÄ

---

### 5 - üê≥ Running Postgres and pgAdmin with Docker-Compose üêò

Instead of running Postgres and pgAdmin separately using two different Docker commands, we can simplify the process by using a **Docker-Compose** configuration file. This allows us to run both services with minimal parameterization. If you're using **Docker Desktop**, `docker-compose` is already installed. If not, you can install it by following this [installation guide](https://docs.docker.com/compose/install/standalone/). üì•

Once installed, you can proceed to complete the [configuration file](./2_docker_sql/docker-compose.yaml). üõ†Ô∏è

**Pro Tip:** üí° Using **Visual Studio Code** with the **Docker extension** can make this step even easier, as it provides helpful suggestions and auto-completion. 

#### üìù Key Notes:

1. **Volume Mounting for Postgres:** üìÇ
   - When mounting the volume `"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw"` for your database, the `rw` mode stands for **read** and **write**. This allows you to access and modify the database as needed. üîÑ
   - If you created a **local docker volume** for mounting the database, you need to do small changes to **specify the right volume**, also indicating that it is **external**:

   ```yaml
   services:
   pgdatabase:
      image: postgres:13
      environment:
         - POSTGRES_USER=root
         - POSTGRES_PASSWORD=root
         - POSTGRES_DB=ny_taxi
      volumes: 
         - dtc_postgres_volume_local:/var/lib/postgresql/data:rw
      ports:
         - "5432:5432" # mapping local host port to container one
   pgadmin:
      image: dpage/pgadmin4
      environment:
         - PGADMIN_DEFAULT_EMAIL=admin@admin.com
         - PGADMIN_DEFAULT_PASSWORD=root
      volumes:
         - ./data_pgadmin:/var/lib/pgadmin
      ports:
         - "8080:80" # mapping local host port to container one
   volumes:
      dtc_postgres_volume_local:
         external: true 
   ```

2. **Persistent pgAdmin Configuration:** üóÇÔ∏è
   - To make pgAdmin's configuration persistent, create a folder named `data_pgadmin`. Then, change its permissions using the command:
     ```bash
     sudo chown 5050:5050 data_pgadmin
     ```
     - **What does this command do?** ü§î
       - The `sudo chown 5050:5050 data_pgadmin` command changes the ownership of the `data_pgadmin` folder to the user and group with ID `5050`. This is necessary because pgAdmin runs under this specific user ID inside the container. By changing the ownership, you ensure that pgAdmin can read and write to this folder, making the configuration persistent across container restarts. üîí

   - After changing the permissions, mount this folder to `/var/lib/pgadmin` in your Docker-Compose configuration file.


#### üöÄ Running Docker-Compose

Once the configuration is ready, we can start the services by running:
```bash
docker-compose up
```
- This command will create a network for the containers specified in the configuration file, and build and run those containers. 

- To stop `docker-compose`, we press `Ctrl+c`. However, to shut it down properly and remove the network and containers, we should use:
  ```bash
  docker-compose down
  ```

- **Detached Mode:** üï∂Ô∏è
  - It is also possible to run `docker-compose` in detached mode:
    ```bash
    docker-compose up -d
    ```
  - This mode allows to regain control of the terminal after launching the containers. It makes it easier to shut down the services properly later. üéØ

---

### üóÑÔ∏è 6 - SQL Refresher: Let's Dive into Queries! üöÄ

Let's do some queries to flex your SQL muscles üí™ For that, we will explore some key concepts using **Postgres** and **PgAdmin**! First, spin up your containers in detached mode with this command:
```bash
docker-compose up -d
```
**Access PgAdmin here:** üåê [http://localhost:8080/browser/](http://localhost:8080/browser/)  
Connect to your `Docker Localhost` server and open the Query Tool! üî•


#### üîç Basic Exploration
**Quick peek at the `zones` table:**
```sql
Select *
From zones
; -- Overview of taxi zones üó∫Ô∏è
```

**First 100 taxi trips:**
```sql
SELECT * 
FROM yellow_taxi_trips 
LIMIT 100
;  -- Let's see what's cooking! üöñ
```

#### ü§ù Introduction to Joins
**Replace zone IDs with human-readable names:**  
*Using implicit joins:*
```sql
Select tpep_pickup_datetime,
       tpep_dropoff_datetime,
	    total_amount,
	    Concat(zpu."Borough",'/', zpu."Zone") "pickup_loc", -- üéØ Pro tip: Alias columns!
	    Concat(zdo."Borough",'/', zdo."Zone") "dropoff_loc"
From yellow_taxi_trips t,
     zones zpu,
	  zones zdo
Where t."PULocationID" = zpu."LocationID"
  And t."DOLocationID" = zdo."LocationID"
Limit 100
;
```

*Modern explicit JOIN syntax (recommended ‚úÖ):*
```sql
Select tpep_pickup_datetime,
       tpep_dropoff_datetime,
	    total_amount,
	    Concat(zpu."Borough",'/', zpu."Zone") "pickup_loc",
	    Concat(zdo."Borough",'/', zdo."Zone") "dropoff_loc"
From yellow_taxi_trips t
Join zones zpu
  On t."PULocationID" = zpu."LocationID"
Join zones zdo
  On t."DOLocationID" = zdo."LocationID"
Limit 100
;
```

#### üïµÔ∏è Data Quality Checks
**Find missing locations:** 
Let's check if there are `NULL` values for locations in the `yellow_taxi_trips` table:
```sql
Select "PULocationID",
       "DOLocationID"
From yellow_taxi_trips
Where "PULocationID" is Null
   Or "DOLocationID" is Null
;
```
Let's also check if there are any pickup or dropoff location in the trips table that is not present in the `zones` one:
```sql
Select "PULocationID", "DOLocationID"
From yellow_taxi_trips
Where "PULocationID" Not in (Select "LocationID"
					         From zones)
	   Or
      "DOLocationID" Not in (Select "LocationID"
					         From zones)
Limit 137
; -- Orphan locations check !
```

**Let's break things!** üí• We will delete a zone and see the impact:
```sql
Delete From zones
Where "LocationID" = 142 -- Bye-bye Lincoln Square East! üëã
;
```
Now we can clearly get records from each time someone was picked or dropped at the location `142` (Manhattan Lincoln Square East) that we deleted.
Note that, if we display all locations with the right names as done earlier with JOIN statements, we won't get the location `142`:
```sql
Select tpep_pickup_datetime,
       tpep_dropoff_datetime,
	    total_amount,
	    Concat(zpu."Borough",'/', zpu."Zone") "pickup_loc",
	    Concat(zdo."Borough",'/', zdo."Zone") "dropoff_loc"
From yellow_taxi_trips t
Join zones zpu
     On t."PULocationID" = zpu."LocationID"
Join zones zdo
     On t."DOLocationID" = zdo."LocationID"
Limit 100
;
```

That is because `Join` in **SQL** is by default an `inner join`. Inner joins show only observations that are in both tables. To make sure we get all information fom the trips table, with null from others if any, we can use a left join, that keeps all records from the left table:
```sql
Select tpep_pickup_datetime,
       tpep_dropoff_datetime,
	    total_amount,
	    Concat(zpu."Borough",'/', zpu."Zone") "pickup_loc",
	    Concat(zdo."Borough",'/', zdo."Zone") "dropoff_loc"
From yellow_taxi_trips t
Left Join zones zpu
     On t."PULocationID" = zpu."LocationID"
Left Join zones zdo
     On t."DOLocationID" = zdo."LocationID"
Limit 100
;  -- üö© Now shows NULL for deleted zones!
```
Note that there are also **right joins** to keep all records from the right table, and **outer joins** to get all records from both tables, as a combinaton of left and rigth joins. See the illustration below to get a simple undersanding of **SQL joins**.
![Joins illustration](../images/joins.PNG)

#### üìä Aggregation Power Hour!
Now, we will get some aggregates on the data using the `Group By` statement.

**Daily trip analysis:**
 Let's take a look at the number of trips for each day. To get days, we can either truncate the datetimes so to ignore times:
```sql
Select tpep_pickup_datetime,
       tpep_dropoff_datetime,
       DATE_TRUNC('DAY', tpep_dropoff_datetime),
	    total_amount
From yellow_taxi_trips
;
```
Or cast datetimes as a dates, to get days that we can count:
```sql
Select Cast(tpep_dropoff_datetime as DATE) as "day", -- By day üìÖ
	    Count(1)
From yellow_taxi_trips
Group by "day"
Order by "day" ASC
;
```
This allows us to get 37 observations for the moth of January. There are 6 outliers with some dates not from this month. Maybe the January taxi data was not well collected. We can also order the result by the count on each day:
```sql
Select Cast(tpep_dropoff_datetime as DATE),
	    Count(1) "trip_count"
From yellow_taxi_trips
Group by Cast(tpep_dropoff_datetime as DATE)
Order by "trip_count" DESC
;
```
**More aggregate functions:**
```sql
Select Cast(tpep_dropoff_datetime as DATE) as "day",
	    Count(1) "trip_count",
       Max(total_amount) "max_bill", -- üí∞ Show me the money!
       Max(passenger_count) "max_passengers"
From yellow_taxi_trips
Group by "day"
Order by "day" ASC
;
```

**Multi-level grouping:**
Finally, it is possible to group by indices of the output table columns, and to order by many columns:

```sql
Select Cast(tpep_dropoff_datetime as DATE) as "day",
       "DOLocationID",
	    Count(1) "trip_count",
       Max(total_amount) "max_bill",,
       AVG(total_amount) as "avg_fare"  -- ‚ú® New metric!
       Max(passenger_count) "max_clients_1trip"
From yellow_taxi_trips
Group by 1, 2 -- üéì Group by output columns 1 (day) and 2 (location)
Order by "day" asc, "DOLocationID" desc
; -- Day + Dropoff Location Insights üìç
```

Now go query like a pro! üèÜ‚ú®

---

## Google Cloud Platform (GCP) & Terraform







---