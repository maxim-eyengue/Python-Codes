![ML Zoomcamp Illustration](https://github.com/maxim-eyengue/Python-Codes/blob/main/ML_Zoomcamp_2024/zoomcamp.jpg)
---


## ðŸ“š Session 8 Summary - Machine Learning Zoomcamp

### Fashion Classification

In the previous session, we discussed tabular data and explored models such as linear and logistic regression, along with tree-based models. In this session, we'll shift our focus to image data for a multi-class classification project, leveraging neural networks. Specifically, we will build a fashion classification service that allows users to upload an image and receive its category in return (e.g., T-shirts). The dataset we'll use is a subset of the clothing dataset, featuring its 10 most popular classes. The training dataset is organized into 10 folders, each named after a category and containing the corresponding images. Since the dataset has already been split, no need to perform train-test-validation splits.

This lesson is focused on practical implementation. For a deeper understanding of how neural networks function, refer to this [tutorial](https://cs231n.github.io/).

We will be using `TensorFlow` and `Keras` for training the models. `TensorFlow` is a popular deep learning framework that facilitates the development of neural network models. In this session, we will cover using pre-trained models, an overview of neural network theory, and introduce key concepts like transfer learning (where we fine-tune pre-trained models to solve specific problems), parameters, generalization, and data augmentation.

#### Setting up the Environment on Saturn Cloud

Log into Saturn Cloud and create a secret key to push notebooks to GitHub using SSH.

One possibility is to use an SSH key generated from GitHub:
1. If you haven't already, generate an SSH key on your laptop associated with your GitHub account. Follow this [tutorial](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent).
2. After creating a resource, add your public SSH key when prompted, following the steps outlined [here](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/08-deep-learning/01b-saturn-cloud.md).
3. Once the environment is set up, ensure that `TensorFlow` is installed by importing it and checking its version.

Another option is to use the one generated by Saturn Cloud:
1. **Generate SSH Key on Saturn Cloud**:
   - Access the **Account Management** section of the Saturn Cloud interface.
   - Generate a new SSH key if you don't already have one. The platform often provides an option to copy the generated public key directly.

2. **Add SSH Key to GitHub**:
   - Log in to your GitHub account.
   - Navigate to **Settings > SSH and GPG Keys > New SSH Key**.
   - Paste the copied SSH key and save it.

3. **Test SSH Connection**:
   - Open a terminal on Saturn Cloud.
   - Run the following command to test the connection:
     ```bash
     ssh -T git@github.com
     ```
     If everything is set up correctly, you'll see a message confirming authentication:
     ``` 
     Hi <username>! You've successfully authenticated, but GitHub does not provide shell access.
     ```

4. **Clone Repository**:
   - Use the following command to clone the dataset repository:
     ```bash
     git clone git@github.com:alexeygrigorev/clothing-dataset-small.git
     ```
      This will create a local copy of the dataset in your Saturn Cloud environment.


### TensorFlow and Keras Overview

**TensorFlow** is a powerful library primarily designed for deep learning and machine learning tasks. It provides the foundation for building and training complex neural networks. On top of this, **Keras** acts as a high-level API that simplifies the process of creating, training, and deploying neural networks, making TensorFlow more accessible.

##### Installation
To install TensorFlow, you can use either of the following commands, depending on your package manager:
- Using Anaconda:  
  ```bash
  !conda install tensorflow -y
  ```
- Using pip:  
  ```bash
  !pip install tensorflow
  ```

**Note:**  
If you have a GPU and wish to utilize it for accelerated training, additional setup is required to install the appropriate CUDA and cuDNN libraries to integrate TensorFlow with your GPU.

##### Keras in TensorFlow
Previously, Keras was an independent library. Since TensorFlow 2.0, Keras has been integrated into TensorFlow as its default high-level API, allowing seamless usage.


#### Working with Images in TensorFlow

##### Loading and Preprocessing Images
To load an image, the `load_img` function from `tensorflow.keras.preprocessing.image` or `tensorflow.keras.utils` can be used. Images must often be resized to specific dimensions required by neural networks, such as \( 299 \times 299 \), \( 224 \times 224 \), or \( 150 \times 150 \). Resizing ensures the input matches the network's expected format.

After loading an image, you can convert it into a NumPy array using `np.array()`.

##### Understanding Image Representation
Images are typically represented as a 3D array with the following dimensions:
- **Height** (number of pixels vertically)
- **Width** (number of pixels horizontally)
- **Color Channels** (Red, Green, Blue or RGB)

Example:
- An image with a size of \( 150 \times 150 \) would have an array representation with a shape of `(150, 150, 3)`.

For each pixel, the RGB values are stored in the range of `0` to `255`, where:
- **R** (Red) = Intensity of the red channel
- **G** (Green) = Intensity of the green channel
- **B** (Blue) = Intensity of the blue channel

##### Example
A single pixel with the RGB values `[179, 171, 99]` would mean:
- \( R = 179 \)  
- \( G = 171 \)  
- \( B = 99 \)  

##### Data Type
The pixel values are stored in a `numpy.array` of type `uint8` (unsigned 8-bit integer). This data type is ideal for pixel representation since:
- Each pixel value ranges from `0` to `255` (1 byte or 8 bits per color channel).
- Unsigned integers are used because negative pixel values are not meaningful.

In summary, a typical color image has three channels (RGB), each with an 8-bit representation. Combined, this structure ensures precise color representation for every pixel. This standardized format simplifies how neural networks process and analyze image data.

---

### Pre-Trained Convolutional Neural Networks

#### **Pre-trained Models**
Pre-trained models are neural networks that have been trained on large datasets to recognize and classify images. They serve as a robust foundation for various computer vision tasks by providing a general understanding of visual features.

#### **ImageNet Dataset**
- **Source**: [ImageNet Dataset](https://www.image-net.org/)
- **Overview**:  
  - **Training Set**: 1,281,167 images  
  - **Validation Set**: 50,000 images  
  - **Test Set**: 100,000 images  
- **Purpose**: General-purpose image classification.

#### **Models Trained on ImageNet**
- Pre-trained models available via [Keras Applications](https://keras.io/api/applications/) are trained on the ImageNet dataset.
- We focus on the **Xception** model due to its balance of:
  - Size: 88 MB.
  - Parameters: 22,910,480.
  - Performance: High accuracy.
  - Speed: Efficient computation, ideal for setups with limited resources.

#### **Accelerating Training and Inference**
- **GPU Usage**:
  - **Advantages**: 
    - Parallel processing capabilities.
    - Superior performance in matrix operations crucial for neural networks.
  - **Tools**:
    - **SageMaker** or **Saturn Cloud** for cloud-based GPU access.
    - Use kernel `tensorflow2_p36` when leveraging SageMaker.

- **CPU vs. GPU**:
  - GPUs can be up to 8x faster than CPUs for training and inference tasks.

#### **Functions Specific to Xception**
- **`preprocess_input`**: Prepares batches of images for Xception by normalizing and scaling input.
- **`decode_predictions`**: Translates model outputs into human-readable class names with associated probabilities.

#### **Customizing Pre-Trained Models**
- While ImageNet-trained models are excellent for general image classification, they may lack specificity for certain domains, such as clothing, where classes like *t-shirt* may not exist.
- **Solution**:
  - Build on top of pre-trained models by fine-tuning or transferring learning to adapt to specific datasets, enabling better performance for specialized tasks like clothing classification.

---

### Convolutional Neural Networks

CNNs are specialized neural networks primarily designed for processing image data. They consist of various layers, including:

a. **Convolutional Layers**  
   - **Purpose:** Extract feature representations from the input image.
   - **Mechanism:**  
     - **Filters (or Kernels):** Small, learnable matrices that slide (convolve) over the image to calculate similarities between the filter and the image.  
     - **Feature Maps:** Resultant outputs that indicate how the image features match the filters.
     - **Hierarchy of Complexity:**  
       - Lower layers learn basic patterns (e.g., edges, corners).  
       - Upper layers learn complex patterns formed by combining simpler ones.
   - **Learning:** Filters are not manually defined; they are learned by the network during training.

b. **Dense Layers (Fully-Connected Layers)**  
   - **Purpose:** Use the extracted feature vector to make predictions.  
   - **Structure:**  
     - Every input node is connected to every output node.  
     - Mathematically represented as \( Wx \), where \( W \) is the weight matrix and \( x \) is the input vector.  
   - **Output for Classification Tasks:**  
     - **Binary Classification:** Uses the **sigmoid** function to compute probabilities (e.g., probability of being a T-shirt).  
     - **Multiclass Classification:** Uses the **softmax** function, a generalized version of sigmoid.

c. **Pooling Layers**  
   - **Purpose:** Reduce the dimensionality of feature maps, minimizing the number of parameters while retaining important information.  
   - **Example:** **MaxPooling** selects the maximum value in a region of the feature map.
  ![Maxpooling Layer](images/Pool_Hands.png)


#### **Workflow in CNNs**
1. **Input Image:** Sent through convolutional layers.
2. **Feature Extraction:** Filters generate feature maps to represent patterns in the image.
3. **Vector Representation:** Final feature maps are flattened into a vector.
4. **Dense Layers:** This vector is processed to make predictions:
   - Intermediate dense layers refine internal representations.
   - The output layer produces probabilities for each target class.
5. **Prediction:** Probabilities are used to determine the most likely class.


#### **Example Architecture**

![LeNet 5 Architecture](images/Lenet5.png)

**LeNet-5** is a classic CNN architecture for recognizing handwritten digits. It includes convolutional layers, pooling layers, and dense layers to demonstrate this workflow.

---

### Transfer Learning
Transfer learning is a powerful machine learning approach where a pre-trained model on a large dataset (e.g., ImageNet) serves as a foundation for a new, task-specific model. This method can leverage the feature extraction capabilities of convolutional layers from the pre-trained model while customizing the dense layers to suit the new task. Here's a breakdown of the key concepts:

#### Key Concepts

1. **Pre-Trained Model**:  
   Models like `Xception` or `ResNet` are trained on extensive datasets, learning to extract general features from images. These convolutional layers (referred to as the *bottom* layers) are preserved for their ability to identify complex patterns such as edges and textures.

2. **Freezing Layers**:  
   By setting `base_model.trainable = False`, we freeze the convolutional layers, preventing their weights from being updated during training. This retains their learned feature extraction capabilities.

3. **Dense Layers for Fine-Tuning**:  
   The top layers of the pre-trained model (typically fully connected or dense layers) are discarded because they are specific to the original dataset's classification task. New dense layers are added to adapt to the target task.

4. **Image Size and Batch Processing**:  
   - Resizing images to \( 150 \times 150 \) reduces computational costs, making training faster. Models process smaller images approximately 4 times faster than larger ones (e.g., \( 299 \times 299 \)).
   - Using a batch size of 32 ensures efficient memory usage during training.

5. **Vector Representation via Average Pooling**:  
   The output from the convolutional base is a 3D tensor. Average pooling transforms it into a 1D vector representation, which serves as the input for the dense layers.

6. **Optimizer and Loss Functions**:  
   - **Optimizer**: The `Adam` optimizer is commonly used for its efficiency and adaptability. It updates the model's weights using gradient descent.
   - **Loss Functions**: These measure how well the model predicts the target:
     - `MeanSquaredError` for regression tasks.
     - `CategoricalCrossentropy` for multi-class classification.
     - `BinaryCrossentropy` for binary classification.
   - Specifying `from_logits = True` indicates that the loss function should handle raw predictions (logits) directly, ensuring numerical stability. Adding a `softmax` activation converts logits into probabilities, and `from_logits = False` allow the optimizer to use them for loss computation.

7. **Learning Rate**:  
   The learning rate is a critical hyperparameter. It determines the step size for updating weights during optimization. Proper tuning is essential for achieving good results.

#### Practical Workflow for Transfer Learning
1. Import a pre-trained model (e.g., `Xception`) with `include_top = False` to exclude the dense layers.
2. Freeze the convolutional layers by setting `base_model.trainable = False`.
3. Apply the model to the input data to generate feature maps.
4. Use average pooling to create a 1D vector representation from the feature maps.
5. Add custom dense layers to output predictions for the target task.
6. Compile the model with an appropriate optimizer and loss function.
7. Train the model with the resized dataset using an optimal learning rate.

This approach efficiently adapts powerful models to specific tasks, saving time and computational resources while achieving high accuracy.

---

### Adjusting the learning rate

Imagine you have a book, and you want to read it. The *learning rate* represents how fast you can read and absorb its content. If you read the book very quickly, you risk forgetting important parts and struggling to recall key details when you need to apply them. On the other hand, reading slowly allows you to study each concept thoroughly and understand it deeply, ensuring better retention. However, if you read too slowly, you might never finish the book. The goal is to find the right reading pace, or learning rate, that balances comprehension and efficiency. Reading too fast may result in superficial understanding, while reading too slowly might mean not acquiring knowledge quickly enough to meet your goals. By maintaining a moderate, balanced pace, you can maximize understanding and effectively apply what you've learned.  

This analogy relates to training machine learning models. Training a model is like reading a book: you're trying to "learn" from the data. Applying that knowledge during testing or validation corresponds to validating the model. If you train the model too quickly (with a high learning rate), it may overfit, memorizing the training data without generalizing well to new data. If you train it too slowly (with a low learning rate), it may underfit, failing to learn enough patterns from the data. A balanced learning rate ensures the model acquires sufficient knowledge and performs well on both training and validation data.  

To illustrate, we defined a function to build a Convolutional Neural Network (CNN), and fine-tuned its learning parameter based on validation data to optimize model performance.  

---

### Checkpointing

Model saving can be performed after each training epoch or when certain conditions are met. During training, the model's performance is typically evaluated on a validation dataset after each epoch. This process often uses callbacks to execute specific actions, such as saving the model when it demonstrates improved performance.

In Keras, the `keras.callbacks.ModelCheckpoint()` function allows you to save the best model based on a specified metric, such as validation accuracy. The `save_best_only = True` parameter ensures that the model is saved only when its performance improves compared to previous epochs.  

Additionally, format specifiers like `%02d` and `%.3f` in file naming are commonly used:  
- `%02d` ensures integers are displayed as two digits, with leading zeros if the number is less than 10.  
- `%.3f` formats a floating-point number with three decimal places.  

This approach optimizes storage and ensures only the most effective model is retained.  

---

### Adding more Layers

In the dense layers of a Convolutional Neural Network (CNN), an intermediate layer can be added after the vector representation to perform additional processing before reaching the output layer. This often enhances the model's capability, as the intermediate layer learns internal representations from the vector representation, which are beneficial for the specific task at hand. Typically, adding one or two such layers can improve results.  

At the end of a neural network, especially for multi-class classification tasks, a `softmax` activation function is commonly used. This function takes the raw scores (logits) from the dense layers and transforms them into probabilities. In our use case, we worked directly with logits rather than applying `softmax`, but the implication remains that `softmax` could be used to convert the raw outputs of the model into proper probabilities.  

Intermediate layers also require activation functions to enhance their performance. For these layers, we use the `ReLU` (Rectified Linear Unit) activation function, which is one of the most widely used activations. It is defined as:  
\[
\text{ReLU}(x) = \max(0, x)
\]  

For output layers, common activation functions include `sigmoid` and `softmax` for classification problems, while intermediate layers can use `ReLU`, `tanh`, and others depending on the use case.  

The size of the intermediate layer is a hyperparameter that must be carefully tuned to optimize model performance.

To monitor GPU utilization during training, you can open a terminal and use the following commands:  
- `nvidia-smi`: Displays the current GPU usage.  
- `watch nvidia-smi`: Continuously tracks GPU usage by running `nvidia-smi` every 2 seconds and displaying the results.

Finally, note that adding extra layers increases the model's complexity. This requires a balance, as overly complex models may lead to overfitting or increased computational costs.  

---

### Regularization and Dropout

Dropout is a powerful regularization method used in neural networks to prevent overfitting. In Convolutional Neural Networks (CNNs), training for many epochs can lead to the model memorizing specific details, such as logos in clothing images, to make classifications. This is undesirable in tasks like clothing classification, where the focus should be on general features like the overall shape of the garment, rather than specific, irrelevant details.

The main idea behind dropout is to randomly deactivate a subset of neurons in the network during training. This prevents the model from relying too heavily on specific neurons or features. By "dropping out" certain nodes in the inner layers at random, the network is forced to learn more robust and generalized patterns.

Here's how it works:
- During each iteration of training, a fraction of neurons in the layer are randomly "frozen," meaning their weights are not updated.
- This process effectively masks different parts of the input to the inner layer during each iteration, encouraging the network to distribute its learning across all neurons rather than overfitting to specific features.

Dropout rate, which represents the fraction of neurons to deactivate, is a hyperparameter that can be fine-tuned to achieve optimal results. Regularization through dropout helps the model generalize better to unseen data by preventing it from learning patterns that don't exist in the broader dataset.

---

### Data Augmentation

#### **Why Use Data Augmentation?**
Data augmentation is a technique to address **overfitting** by artificially increasing the size and diversity of the training dataset. It helps the model generalize better by introducing variations in the data while preserving the semantic meaning of the images.

#### **Common Transformations for Images:**
1. **Flip**:
   - Horizontal: Mirror the image along the vertical axis.
   - Vertical: Mirror the image along the horizontal axis.
   - Both: Apply both flips simultaneously.
   - **Example Use:** Symmetrical objects, such as clothes.

2. **Rotation**:
   - Apply a rotation angle (positive for clockwise, negative for counterclockwise).
   - **Example Use:** When object orientation does not matter (e.g., aerial imagery).

3. **Shift**:
   - Horizontal: Move the image left (negative) or right (positive).
   - Vertical: Move the image up (positive) or down (negative).
   - **Example Use:** Non-centered objects in datasets.

4. **Shear**:
   - Skew the image in a given direction (left or right).
   - **Example Use:** Perspective transformations in scenes with depth.

5. **Zoom**:
   - Zoom in: `value < 1`
   - Zoom out: `value > 1`
   - **Example Use:** Images with varying focus or distance from objects.

6. **Brightness and Contrast Adjustments**:
   - Alter the intensity or contrast of the image.
   - **Example Use:** Images captured in different lighting conditions.

7. **Black Patching**:
   - Mask parts of the image to simulate occlusions.
   - **Example Use:** Testing the model's ability to recognize partial objects.

#### **Choosing Transformations**:
- **Judgement-Based**: Analyze the dataset's characteristics and apply transformations that align with its variability.
   - Example: Avoid horizontal flips for text datasets where direction matters.
- **Dataset Analysis**: 
   - Are objects centered or scattered? Use rotation and shift accordingly.
   - Are images symmetric? Use vertical flips if applicable.
- **Hyperparameter Tuning**:
   - Experiment by training the model for 10â€“20 epochs with specific transformations.
   - Evaluate if the transformations improve, maintain, or worsen the model performance.

#### **Important Notes**:
1. **Validation Set**:
   - Do not augment the validation dataset. Consistency is critical for comparing model performance.
2. **CPU-GPU Workflow**:
   - The **CPU** handles augmentation while the **GPU** fits the model. They can work in parallel using tools like `tensorflow.data` to build efficient data pipelines.
3. **Reproducibility**:
   - Achieving consistent results in neural networks is challenging due to inherent randomness (e.g., weight initialization, random shuffling).

#### **Tools for Data Augmentation**:
- **Keras**: Use `ImageDataGenerator()` for simple pipelines.
- **TensorFlow**: Leverage `tf.data` for advanced, efficient augmentation pipelines.

#### **When Does Augmentation Make Sense?**
- Augmentation is particularly beneficial for high-resolution images with intricate details, as they provide more variation opportunities. Conversely, simpler datasets or small images may not benefit significantly.

---

### Training a Larger Model

When training a larger model, increasing the input size, such as from 150x150 to 299x299, 
makes the model more computationally intensive, often multiplying the training time by around 4.
This is because the number of parameters and the complexity of the data increase with the larger input size,
requiring more time to process. That is why smaller models are used for experimentations.

To address overfitting and improve accuracy when the model's performance isn't consistently improving, there are a few strategies that can be employed:

1. **Data Augmentation**: This technique involves artificially increasing the size of the training dataset by applying random transformations (like rotation, flipping, zooming, or shifting) to the original images. This helps the model generalize better and reduces overfitting by providing it with more diverse data during training.

2. **Reduce the Learning Rate**: When training larger models, a high learning rate might lead to unstable training, causing the accuracy to "jump" rather than steadily improve. By reducing the learning rate, you can encourage more gradual adjustments to the model weights, allowing for a more stable and effective convergence, especially when you're working with larger models that can have more volatile gradients.

3. **Regularization**: Techniques like **Dropout** or **L2 regularization (weight decay)** can help reduce overfitting. Dropout randomly drops units during training, forcing the network to learn more robust features, while L2 regularization penalizes large weights, encouraging simpler models that avoid overfitting.

4. **Early Stopping**: Monitoring validation accuracy and stopping training once it stops improving can prevent overfitting. This ensures that the model doesn't continue to learn noise or irrelevant patterns after it has already achieved optimal performance.

5. **Batch Normalization**: This helps stabilize and speed up the training by normalizing activations in each layer, reducing the likelihood of overfitting and improving generalization.

By combining these techniques, you can better manage the trade-off between model complexity,
training time, and overfitting, improving the model's ability to generalize to unseen data.

---

### Using the Model

Saving our model in **Keras format** ensures that it contains:

1. **Model Architecture**: The structure of the layers and their configuration.
2. **Weights**: The learned parameters of the model.
3. **Compile Information**: The loss function, optimizer, and metrics used during training.

This comprehensive saving allows for seamless loading and further use of the model. 

#### Key Notes:
- Ensure that the preprocessing pipeline (e.g., resizing, normalization) is consistent across training, validation, and testing.
- Use `model.evaluate()` for bulk evaluation and `model.predict()` for individual predictions.
- If your model was trained with specific callbacks or custom layers, make sure they are included during loading for accurate reproduction. 

---

## **ðŸ’¡ Key Takeaways**

The goal of this project was to develop a solution for categorizing clothing items using image classification. We leveraged `Keras` with pre-trained models as a foundation and fine-tuned the architecture by adding layers and optimizing their sizes. 

To address overfitting, we incorporated techniques such as dropout and data augmentation. Initial experimentation was conducted with a smaller model, which provided valuable insights before scaling up to train a larger, more robust model.

The final model was rigorously evaluated to ensure it did not overfit the data. It's worth noting that alternatives like `PyTorch` or `MXNet` could also be used in place of `TensorFlow` and `Keras` for similar implementations.

---
